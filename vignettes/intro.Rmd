---
title: "Introduction to StatComp20095"
author: '20095'
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp18024}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp20095__ is a simple R package developed to present the homework of statistic computing course by author 20095 and three additional functions.

1.EM2: A function to estimate the two-dimensional joint density with gaussian mixture model by EM algorithm.

2.dic.vs.new.R: A function to compare the iterative speed of solution of dichotomy method and Newton method.

3.chisq_test2: Make a faster version of the chi-square statistic of the cross-contingency table to verify the independence between two variables.

## EM2 function
***Principle of functional algorithm***
Suppose that the density of the gaussian mixture model is
$$f(x)=\sum_{i=1}^{C}\pi_i\phi(x;\mu_i,\Sigma_i),$$
where,$\phi(x;\mu_i,\Sigma_i)$ is the density of 2-dim Gaussian . Set$\{\pi_i,\mu_i,\Sigma_i\},i=1,...,C$are parameters. Let $x_j,j=1,...,n,$ be $n$ sample, $z_{ij}=P(x_j \mbox{ belong to the } i-th\mbox{ mixing part})i=1,...,C,j=1,...,n$.

At iteration time t, given $\pi_i,\mu_i,\Sigma_i,i=1,...,C$

* E-step
\begin{align*}
z_{kl}^{t}&=E[z_{kl}|\pi_i^t,\mu_i^t,\Sigma_i^t]\\
&=P(x_j \mbox{ belong to the } i-th\mbox{ mixing part}|\pi_i^t,\mu_i^t,\Sigma_i^t,x_l,i=1,...,C)\\
&=\frac{\pi_k^t\phi(x_l;\mu_k^t,\Sigma_k^t)}{\sum_{i=1}^{C}\pi_i^t\phi(x_l;\mu_i^t,\Sigma_i^t)}.
\end{align*}
* M-step
\begin{align*}
\pi_i^{t+1}&=\frac{1}{n}\sum_{j=1}{n}z_{ij}^t\\
\mu_i^{t+1}&=\frac{\sum_{j=1}^nz_{ij}^tx_j}{\sum_{j=1}^nz_{ij}^t},\\
\Sigma_i^{t+1}&=\frac{\sum_{j=1}^nz_{ij}^t(x_j-\mu_i^{j+1})(x_j-\mu_i^{j+1})^T}{\sum_{j=1}^nz_{ij}^t}.
\end{align*}

The source R code for _EM2_ is as follows:
```{r}
EM2 <- function(data,mu, Sig,tol){
  C=ncol(mu)
  data=as.matrix(data)
  n=nrow(data);p=ncol(data)
  Z <- matrix(c(rep(1, n), rep(0, (n - 1) * C)), n, C)
  mu0=mu
  pi <- rep(1 / C, C)
  dif=1;iter=0
  while (dif >tol) { 
    for (k in 1:n) { 
      for (l in 1:C) { 
        Z[k,l] <- Z[k,l] <- pi[l] * dmvnorm(data[k,], mu[l,], Sig[(p * (l - 1) + 1):(p * l),])
      }
      Z[k,] <- Z[k,] / sum(Z[k,]) 
    } 
    # update Z (E-step) 
    pi <- colMeans(Z) 
    # update pi (M-step-1) 
    for (i in 1:C) { 
      mu[i,] <- t(Z[,i]) %*% data / sum(Z[,i]) 
      # update mu (M-step-2) 
      sumsig <- Z[1,i] * (data[1,] - mu[i,]) %*% t(data[1,] - mu[i,]) 
      for (k in 2:n) { 
        sumsig <- sumsig + Z[k,i] * (data[k,] - mu[i,]) %*% t(data[k,] - mu[i,]) 
      }
      Sig[(p * (i - 1) + 1):(p * i),] <- sumsig / sum(Z[,i]) 
      # update Sigma (M-step-3) 
    }
    dif=max(mu-mu0)
    mu0=mu
    iter=iter+1
  }
  #result of EM
  list("mu"=mu,'Sigma'=Sig,'lambda'=pi,'iters'=iter)
}

#example
library(mvtnorm)
data(faithful)
mu <- matrix(c(2, 50, 4, 80),2, 2, byrow = T) 
Sig <- matrix(rep(c(1, 0, 0, 1), 2), ncol = 2, byrow = T)
EM2(faithful,mu, Sig,tol=1e-3)
#compare with mixtools package
library(mixtools)
mv <- mvnormalmixEM(faithful)
list("mu"=mv$mu,'Sigma'=mv$sigma,'lambda'=mv$lambda)
```

***Benchmarking _EM2_ and _mvnormalmixEM_***
```{r}
library(microbenchmark)
suppressWarnings(
tm <- microbenchmark(
  EM2 = EM2(faithful,mu, Sig,tol=1e-3),
  mv <- mvnormalmixEM(faithful)
))
knitr::kable(summary(tm)[,c(1,3,5,6)])
```

Analysis of results:

The results of the EM2 function are consistent with those of the mvnormalmixEM function in the MixTools package, and the computation time can be reduced to nearly one-tenth of that of the mvnormalmixEM function.


## dic.vs.new function

The source R code for _dic.vs.new_ is as follows:
```{r}
dic.vs.new <- function(f,fgrad,xrange,x0,eps,it_max){
  #dichotomy
  start=xrange[1];end=xrange[2]
  dichotomy<-function(f,xrange,start,end,eps=1e-5,it_max=1000){
    i=0;a=xrange[1];b=xrange[2]
    while(i<=it_max){
      if(f(a)*f(b)>0)
        list(fail="find root is fail!")
      else{
        repeat{
          if(abs(b-a)<eps) break;
          x<-(a+b)/2
          if(f(a)*f(x)<0) b<-x else a<-x
          i=i+1
        }
      }
      return(list(x,i) )#root and times of iterations
    }
  }
  #Newton
  funs=function(f,fgrad,x){
    f=x
    J=f(x)/fgrad(x)
    list(f=f,J=J);
  }
  Newtons=function(fun,f,fgrad,x,eps=1e-5,it_max=1000){
    index=0;k=1
    while(k<=it_max){
      norm=abs(fun(f,fgrad,x)$J)
      x=x-fun(f,fgrad,x)$J
      if(norm<eps){ index=1;break}
      k=k+1
    }
    if(index==0) list(fail="find root is fail!")
    return(c(x,k))#root and times of iterations
  }
  #Compare the two algorithms
  result_dic=dichotomy(f,xrange,eps,it_max)
  result_new=Newtons(funs,f,fgrad,x0,eps,it_max)
  result=cbind(dichotomy=result_dic,Newtons=result_new)
  rownames(result)=c("root","iters")
  return(result)
}

#example
#The equation that needs to be solved.
f=function(x){
  2*x^3-4*x^2+3*x-6
}
#The first derivative of the equation to be solved.
fgrad=function(x){
  6*x^2-8*x+3
}
dic.vs.new(f,fgrad,xrange=c(0,3),x0=1,eps=1e-5,it_max=1000)
```

Analysis of results:

It can be found that in the example, Newton's method only iterates for 9 times before finding an approximate solution, while the dichotomy iterates for 19 times. The iteration speed of Newton's method in the example is twice as high as that of the dichotomy.


## chisq_test function

The value of the test-statistic is   
  $$\chi ^{2}=\sum _{i=1}^{r}\sum _{j=1}^{c}\frac{(O_{i,j}-E_{i,j})^{2}}{E_{i,j}}$$
  $$E_{i,j} = N \cdot p_{i,j} = N \cdot p_{i\cdot} \cdot p_{\cdot j} = \frac{E_{i\cdot} \cdot E_{\cdot j}}{N}$$
here, c = 2 because the input is two numeric vectors with no missing values.

The source R code for _chisq_test_ is as follows:
```{r}
chisq_test <- function(x){
  m <- nrow(x);  n <- ncol(x); N <- sum(x)
  E <- matrix(0,m,n)
  rowsums <- unlist(lapply(1:m, function(i) sum(x[i,]))) # which is used to computed pi.
  colsums <- unlist(lapply(1:n, function(j) sum(x[,j])))
  for (i in 1:m){
    for (j in 1:n) {
      E[i,j] <- rowsums[i]*colsums[j]/N#Compute the number if the calculation is independent.
    }
  }
  # degree of freedom
  df <- (m-1) * (n-1)
  #Calculate the chi-square statistic
  chi_sqr <- sum((x-E)^2/E) 
  p_value <- dchisq(chi_sqr, df = df)
  (test <- list(chi_sqr = chi_sqr, df = df, p_value = p_value))
}

#example
M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
dimnames(M) <- list(gender = c("F", "M"),party = c("Democrat","Independent", "Republican"))
(M)
(Xsq <- chisq.test(M))
chisq_test(M)
```

***Benchmarking _chisq_test_ and _chisq.test_***
```{r}
library(microbenchmark)
print(microbenchmark::microbenchmark(chisq_test(M),chisq.test(M)))
tm <- microbenchmark(
  ct1 = chisq_test(M),
  ct2 <- chisq.test(M)
)
knitr::kable(summary(tm)[,c(1,3,5,6)])
```

Analysis of results:

We can say that calling *chisq_test()* quite a bit faster than calling *chisq.test()* for two numeric vectors with no missing values.
